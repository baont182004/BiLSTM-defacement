import requests
from bs4 import BeautifulSoup
import json
import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# --- C·∫§U H√åNH ---
DEFACED_URL_FILE = 'defacement_url.txt'
NORMAL_URL_FILE = 'normal_url.txt'
OUTPUT_JSON_FILE = 'rawData.json'
MAX_WORKERS = 10  # S·ªë l∆∞·ª£ng URL x·ª≠ l√Ω song song (tƒÉng/gi·∫£m t√πy theo m·∫°ng)
REQUEST_TIMEOUT = 10 # Th·ªùi gian (gi√¢y) ch·ªù m·ªói URL
# --------------------

# Header ƒë·ªÉ gi·∫£ l·∫≠p tr√¨nh duy·ªát, tr√°nh b·ªã ch·∫∑n
REQUEST_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

def read_urls_from_file(filepath):
    """ƒê·ªçc URL t·ª´ file .txt, m·ªói d√≤ng m·ªôt URL."""
    if not os.path.exists(filepath):
        print(f"L·ªñI: Kh√¥ng t√¨m th·∫•y t·ªáp {filepath}")
        return []
    with open(filepath, 'r', encoding='utf-8') as f:
        urls = [line.strip() for line in f if line.strip().startswith('http')]
    return urls

def extract_text_from_url(url, label):
    """
    T·∫£i URL, tr√≠ch xu·∫•t vƒÉn b·∫£n thu·∫ßn t√∫y (kh√¥ng c√≥ HTML, script)
    v√† tr·∫£ v·ªÅ m·ªôt dictionary.
    """
    try:
        # T·∫£i n·ªôi dung trang
        response = requests.get(url, headers=REQUEST_HEADERS, timeout=REQUEST_TIMEOUT, verify=False)
        response.raise_for_status() # B√°o l·ªói n·∫øu m√£ http l√† 4xx ho·∫∑c 5xx

        # S·ª≠ d·ª•ng BeautifulSoup ƒë·ªÉ ph√¢n t√≠ch HTML
        soup = BeautifulSoup(response.content, 'html.parser')

        # X√≥a c√°c th·∫ª script v√† style (m√£ nh√∫ng)
        for script_or_style in soup(["script", "style"]):
            script_or_style.decompose()

        # L·∫•y vƒÉn b·∫£n thu·∫ßn
        raw_text = soup.get_text()

        # L√†m s·∫°ch vƒÉn b·∫£n: x√≥a xu·ªëng d√≤ng, tab v√† kho·∫£ng tr·∫Øng th·ª´a
        cleaned_text = " ".join(raw_text.split()).strip()

        if cleaned_text:
            return {
                "url": url,
                "label": label,
                "text": cleaned_text
            }
        else:
            # Tr·∫£ v·ªÅ None n·∫øu trang kh√¥ng c√≥ vƒÉn b·∫£n
            return None 

    except requests.RequestException as e:
        # Ghi l·∫°i l·ªói nh∆∞ng kh√¥ng d·ª´ng ch∆∞∆°ng tr√¨nh
        # print(f"L·ªói khi x·ª≠ l√Ω {url}: {e}")
        return None

def main():
    print("--- B·∫ÆT ƒê·∫¶U B∆Ø·ªöC 1 (PHI√äN B·∫¢N PYTHON) ---")
    print("ƒêang tr√≠ch xu·∫•t vƒÉn b·∫£n thu·∫ßn t·ª´ URL...")

    # 1. ƒê·ªçc danh s√°ch URL
    defaced_urls = read_urls_from_file(DEFACED_URL_FILE)
    normal_urls = read_urls_from_file(NORMAL_URL_FILE)
    
    if not defaced_urls or not normal_urls:
        print("L·ªñI: C·∫ßn c·∫£ hai t·ªáp URL 'deface' v√† 'normal'. D·ª´ng l·∫°i.")
        return

    print(f"T√¨m th·∫•y {len(defaced_urls)} URL deface v√† {len(normal_urls)} URL b√¨nh th∆∞·ªùng.")
    
    # G·ªôp 2 danh s√°ch l·∫°i v√† g√°n nh√£n
    tasks = [(url, 1) for url in defaced_urls] + [(url, 0) for url in normal_urls]
    
    all_data = []
    
    # 2. X·ª≠ l√Ω song song
    print(f"ƒêang x·ª≠ l√Ω {len(tasks)} URL (s·ª≠ d·ª•ng t·ªëi ƒëa {MAX_WORKERS} lu·ªìng)...")
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # T·∫°o c√°c future
        future_to_task = {executor.submit(extract_text_from_url, url, label): (url, label) for url, label in tasks}
        
        # D√πng tqdm ƒë·ªÉ hi·ªÉn th·ªã thanh ti·∫øn tr√¨nh
        for future in tqdm(as_completed(future_to_task), total=len(tasks), desc="ƒêang c√†o d·ªØ li·ªáu"):
            result = future.result()
            if result:
                all_data.append(result)

    # 3. L∆∞u k·∫øt qu·∫£
    print(f"\nƒê√£ tr√≠ch xu·∫•t th√†nh c√¥ng {len(all_data)} / {len(tasks)} m·∫´u.")
    
    with open(OUTPUT_JSON_FILE, 'w', encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)
        
    print(f"üéâ ƒê√£ l∆∞u 'VƒÉn b·∫£n th√¥' v√†o t·ªáp: {OUTPUT_JSON_FILE}")
    print("B√¢y gi·ªù b·∫°n c√≥ th·ªÉ ti·∫øn h√†nh B∆∞·ªõc 2: Tokenization.")

if __name__ == "__main__":
    # T·∫Øt c·∫£nh b√°o v·ªÅ vi·ªác kh√¥ng x√°c th·ª±c SSL (verify=False)
    requests.packages.urllib3.disable_warnings() 
    main()