import os
import json
import numpy as np
import subprocess
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import warnings

# --- C·∫§U H√åNH (ƒê√É T·ªêI ∆ØU T·ªêC ƒê·ªò) ---
DEFACED_URL_FILE = 'defacement_url.txt'
NORMAL_URL_FILE = 'normal_url.txt'
OUTPUT_JSON_FILE = 'rawData.json'
SCRAPER_JS_FILE = 'get_text_puppeteer.js' 
MAX_WORKERS = 10     # <-- TƒÇNG S·ªê LU·ªíNG SONG SONG (TƒÉng l√™n 15 ho·∫∑c 20 n·∫øu m√°y b·∫°n m·∫°nh)
PROCESS_TIMEOUT = 25 # Gi·∫£m m·ªôt ch√∫t (t·ª´ 30)
REQUEST_TIMEOUT = 8  # Gi·∫£m m·ªôt ch√∫t (t·ª´ 10)
REQUEST_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# --- 2. Logic C√†o d·ªØ li·ªáu (Hybrid) ---

# Ph∆∞∆°ng ph√°p 1 (∆Øu ti√™n): G·ªçi Node.js/Puppeteer
def extract_text_primary(url):
    try:
        command = ['node', SCRAPER_JS_FILE, url]
        result = subprocess.run(
            command, capture_output=True, text=True, encoding='utf-8',
            timeout=PROCESS_TIMEOUT
        )
        if result.returncode != 0:
            return None 
        return result.stdout.strip()
    except Exception:
        return None

# Ph∆∞∆°ng ph√°p 2 (D·ª± ph√≤ng): D√πng "curl" (Requests + BeautifulSoup)
def extract_text_fallback(url):
    try:
        response = requests.get(url, headers=REQUEST_HEADERS, timeout=REQUEST_TIMEOUT, verify=False)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        for script_or_style in soup(["script", "style"]):
            script_or_style.decompose()
        raw_text = soup.get_text()
        return " ".join(raw_text.split()).strip()
    except Exception:
        return None

# H√†m x·ª≠ l√Ω cho m·ªói URL
def process_url(task):
    url, label = task
    
    text = extract_text_primary(url) # ∆Øu ti√™n Puppeteer
    source = "Puppeteer (JS)"
    
    if text is None: # N·∫øu Puppeteer th·∫•t b·∫°i
        text = extract_text_fallback(url) # Th·ª≠ d√πng 'curl'
        source = "Requests (curl)"
    
    if text: 
        return { 'url': url, 'label': label, 'text': text, 'source': source }
    return None

# --- H√ÄM CH√çNH ---
def main():
    print("--- B·∫ÆT ƒê·∫¶U B∆Ø·ªöC 1 (PHI√äN B·∫¢N HYBRID) ---")
    
    def read_urls(filepath, label):
        if not os.path.exists(filepath):
            print(f"L·ªñI: Kh√¥ng t√¨m th·∫•y t·ªáp {filepath}")
            return []
        with open(filepath, 'r', encoding='utf-8') as f:
            urls = [line.strip() for line in f if line.strip().startswith('http')]
        return [(url, label) for url in urls]

    tasks = read_urls(DEFACED_URL_FILE, 1) + read_urls(NORMAL_URL_FILE, 0)
    
    if not tasks:
        print("L·ªñI: Kh√¥ng t√¨m th·∫•y URL n√†o ƒë·ªÉ x·ª≠ l√Ω. D·ª´ng l·∫°i.")
        return

    print(f"T√¨m th·∫•y t·ªïng c·ªông {len(tasks)} URL. B·∫Øt ƒë·∫ßu x·ª≠ l√Ω v·ªõi {MAX_WORKERS} lu·ªìng...")
    all_data = []

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(process_url, task) for task in tasks]
        
        for future in tqdm(as_completed(futures), total=len(tasks), desc="ƒêang c√†o d·ªØ li·ªáu"):
            result = future.result()
            if result:
                all_data.append(result)
    
    with open(OUTPUT_JSON_FILE, 'w', encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)

    print(f"\n--- HO√ÄN T·∫§T B∆Ø·ªöC 1 ---")
    print(f"üéâ ƒê√£ l∆∞u {len(all_data)} / {len(tasks)} m·∫´u th√†nh c√¥ng v√†o file {OUTPUT_JSON_FILE}")

if __name__ == "__main__":
    warnings.filterwarnings('ignore', message='Unverified HTTPS request')
    main()